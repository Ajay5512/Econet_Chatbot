import os
import logging
from tqdm.auto import tqdm
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from elasticsearch import Elasticsearch
from groq import Groq


load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Initialize model and Elasticsearch client
model_name = os.getenv('MODEL_NAME')
es_host = os.getenv('ELASTIC_URL_LOCAL')
model = SentenceTransformer(model_name)
es_client = Elasticsearch(es_host)

def elastic_search_knn(field, vector, index_name="customer-support"):
    """
    Perform a k-nearest neighbors (k-NN) search in Elasticsearch.

    Args:
        field (str): The field in Elasticsearch where the vector is stored.
        vector (list): The vector to search for.
        index_name (str, optional): The Elasticsearch index name. Defaults to 'customer-support'.

    Returns:
        list: A list of documents (questions and answers) that are most similar to the input vector.
    """
    logging.info(f"Performing k-NN search on field '{field}' in index '{index_name}'")
    knn = {
        "field": field,
        "query_vector": vector,
        "k": 5,
        "num_candidates": 10000
    }
    search_query = {
        "knn": knn,
        "_source": ["question", "answer"]
    }
    
    es_results = es_client.search(
        index=index_name,
        body=search_query
    )
    logging.info("k-NN search completed.")
    
    return [hit['_source'] for hit in es_results['hits']['hits']]

def question_answer_vector_knn(question):
    """
    Encode a question using the SentenceTransformer model and perform k-NN search in Elasticsearch.

    Args:
        question (str): The question to be encoded and searched.

    Returns:
        list: A list of documents (questions and answers) similar to the input question.
    """
    logging.info(f"Encoding question: {question}")
    v_q = model.encode(question)
    return elastic_search_knn('question_answer_vector', v_q)

def build_prompt(query, search_results):
    """
    Build a customer support prompt based on the search results.

    Args:
        query (str): The user's question.
        search_results (list): The search results (similar questions and answers).

    Returns:
        str: The formatted prompt to be used for the LLM.
    """
    logging.info("Building prompt for LLM.")
    prompt_template = """
You are a highly knowledgeable, friendly, and empathetic customer support assistant for a telecommunications company.
Your role is to assist customers by answering their questions with accurate, clear, and concise information based on the CONTEXT provided. 
Please respond in a conversational tone that makes the customer feel heard and understood. Be concise, professional, and empathetic in your responses. 
Use only the facts from the CONTEXT when answering the customer's QUESTION.
If the CONTEXT does not have the information to answer the QUESTION, gently suggest that the customer reach out to a live support agent for further assistance.

QUESTION: {question}

CONTEXT: 
{context}
""".strip()
    
    context = "\n\n".join([f"question: {doc['question']}\nanswer: {doc['answer']}" for doc in search_results])
    return prompt_template.format(question=query, context=context).strip()

def llm(prompt, model='llama-3.2-90b-text-p'):
    """
    Interact with the LLM to get a response based on the prompt.

    Args:
        prompt (str): The prompt built from the user's question and the search results.
        model (str, optional): The LLM model name. Defaults to 'llama-3.2-90b-text-p'.

    Returns:
        str: The response generated by the LLM.
    """
    logging.info(f"Sending prompt to LLM: {model}")
    client = Groq()
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}]
    )
    logging.info("Received response from LLM.")
    
    return response.choices[0].message.content

def rag(query, model='llama-3.1-70b-versatile') -> str:
    """
    Perform a retrieval-augmented generation (RAG) operation to answer a question.

    Args:
        query (str or dict): The user's query, either a string or a dictionary with 'Question' or 'question' key.
        model (str, optional): The LLM model name. Defaults to 'llama-3.1-70b-versatile'.

    Returns:
        str: The final answer generated by the RAG process.
    """
    question = query if isinstance(query, str) else query.get('Question') or query.get('question')
    if not question:
        raise ValueError("Input must be a string or a dictionary with a 'Question' or 'question' key")
    
    logging.info(f"Processing RAG for question: {question}")
    search_results = question_answer_vector_knn(question)
    prompt = build_prompt(question, search_results)
    answer = llm(prompt, model=model)
    logging.info("RAG process completed.")
    
    return answer

def get_answer_for_question(question):
    """
    Get the answer for a user's question using RAG (retrieval-augmented generation).

    Args:
        question (str): The user's question.

    Returns:
        str: The generated answer for the user's question.
    """
    return rag(question)

if __name__ == "__main__":
    # Set up the Groq API key from .env
    os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")
    
    custom_question = input("Enter your question: ")
    logging.info(f"User question: {custom_question}")
    
    answer = get_answer_for_question(custom_question)
    
    print(f"\nQuestion: {custom_question}")
    print(f"Answer: {answer}")
